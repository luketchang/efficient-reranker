{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_hits_from_qrels_queries_corpus(qrels_file, queries_file, corpus_file=None):\n",
    "    print(f\"Loading qids from '{queries_file}'\")\n",
    "    queries = load_qids_to_queries(queries_file)\n",
    "\n",
    "    print(f\"Loading corpus from '{corpus_file}'\")\n",
    "    corpus = load_pids_to_passages(corpus_file) if corpus_file is not None else None\n",
    "\n",
    "    # Step 3: Load qrels and combine all data\n",
    "    results = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip if the first line is the header\n",
    "            if line.startswith(\"query-id\"):\n",
    "                continue\n",
    "\n",
    "            qid, docid, score = line.strip().split('\\t')\n",
    "            score = float(score)\n",
    "\n",
    "            # Initialize query entry if not already present\n",
    "            if qid not in results:\n",
    "                results[qid] = {'query': queries[qid], 'hits': []}\n",
    "\n",
    "            # Create a hit entry\n",
    "            hit = {\n",
    "                'qid': qid,\n",
    "                'docid': docid,\n",
    "                'score': score,\n",
    "                'content': corpus[docid] if corpus_file is not None else None\n",
    "            }\n",
    "\n",
    "            results[qid]['hits'].append(hit)\n",
    "\n",
    "    # Step 4: Sort the queries by numeric qid and their hits by score\n",
    "    rank_results = []\n",
    "    for qid in sorted(results.keys(), key=lambda x: int(x.replace(\"test\", \"\").replace(\"train\", \"\").replace(\"dev\", \"\"))):  # Sort by numeric qid\n",
    "        sorted_hits = sorted(\n",
    "            results[qid]['hits'], \n",
    "            key=lambda x: -x['score']  # Sort hits by score in descending order\n",
    "        )\n",
    "        rank_results.append({\n",
    "            'query': results[qid]['query'],\n",
    "            'hits': sorted_hits\n",
    "        })\n",
    "\n",
    "    return rank_results\n",
    "\n",
    "def load_qids_to_queries(queries_file):\n",
    "    queries = {}\n",
    "    with open(queries_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            qid, query = line[\"_id\"], line[\"text\"]\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "\n",
    "def load_pids_to_passages(corpus_file):\n",
    "    corpus = {}\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            pid = data[\"_id\"]\n",
    "            \n",
    "            # Extract title and text, combining them if the title exists\n",
    "            title = data.get(\"title\", \"\")\n",
    "            text = data[\"text\"]\n",
    "            passage = title + \"\\n\" + text if title and title.strip() else text\n",
    "            \n",
    "            corpus[pid] = passage\n",
    "    return corpus\n",
    "\n",
    "def load_qid_to_pid_to_score(qrels_file):\n",
    "    qid_to_pid_to_score = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"query-id\"):\n",
    "                continue\n",
    "\n",
    "            qid, pid, score = line.strip().split('\\t')\n",
    "            score = float(score)\n",
    "            \n",
    "            if qid not in qid_to_pid_to_score:\n",
    "                qid_to_pid_to_score[qid] = {}\n",
    "            qid_to_pid_to_score[qid][pid] = score\n",
    "    return qid_to_pid_to_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from data_utils import load_qid_to_pid_to_score, load_pids_to_passages, load_hits_from_qrels_queries_corpus, strip_prefixes\n",
    "import random\n",
    "\n",
    "class PositiveNegativeDataset(Dataset):\n",
    "    def __init__(self, queries_path, corpus_path, negative_rank_results_path, positive_rank_results_path, tokenizer, max_seq_len=None, num_neg_per_pos=8, seed=43):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.positive_rank_results = load_qid_to_pid_to_score(positive_rank_results_path)\n",
    "        self.corpus = load_pids_to_passages(corpus_path)\n",
    "        negative_rank_results = load_hits_from_qrels_queries_corpus(negative_rank_results_path, queries_path, corpus_path)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.truncation = max_seq_len is not None\n",
    "        self.num_neg_per_pos = num_neg_per_pos  # Number of negatives to sample per positive\n",
    "        self.seed = seed  # Global seed for reproducibility\n",
    "        \n",
    "        local_rng = random.Random(seed)\n",
    "        self.negative_rank_results_with_positives = []\n",
    "        for rank_result in negative_rank_results:\n",
    "            hits = rank_result['hits']\n",
    "            qid = hits[0]['qid']\n",
    "            if qid in self.positive_rank_results:\n",
    "                for positive_id in self.positive_rank_results[qid]:\n",
    "                    positive_score = self.positive_rank_results[qid][positive_id]\n",
    "                    \n",
    "                    # Shuffle hits once for each query before creating the dataset\n",
    "                    local_rng.shuffle(hits)\n",
    "                    \n",
    "                    self.negative_rank_results_with_positives.append({\n",
    "                        \"query_id\": qid,\n",
    "                        \"query\": rank_result['query'],\n",
    "                        \"positive_id\": positive_id,\n",
    "                        \"positive_score\": positive_score,\n",
    "                        \"hits\": hits  # All hits for negative sampling\n",
    "                    })\n",
    "\n",
    "        # Create index mapping: [(query_idx, neg_group_idx)]\n",
    "        self.index_mapping = []\n",
    "        for query_idx, rank_result in enumerate(self.negative_rank_results_with_positives):\n",
    "            num_hits = len([hit for hit in rank_result['hits'] if hit['docid'] != rank_result['positive_id']])\n",
    "            num_groups = num_hits // self.num_neg_per_pos\n",
    "            self.index_mapping.extend([(query_idx, group_idx) for group_idx in range(num_groups)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query_idx, group_idx = self.index_mapping[idx]\n",
    "        rank_result = self.negative_rank_results_with_positives[query_idx]\n",
    "        query = rank_result['query']\n",
    "\n",
    "        # Positive passage\n",
    "        positive_id = rank_result['positive_id']\n",
    "        positive_passage = self.corpus[positive_id]\n",
    "\n",
    "        # Determine negative samples for the current group\n",
    "        start_idx = group_idx * self.num_neg_per_pos\n",
    "        end_idx = start_idx + self.num_neg_per_pos\n",
    "        negative_candidates = [hit for hit in rank_result['hits'] if hit['docid'] != positive_id]\n",
    "        hard_negatives = negative_candidates[start_idx:end_idx]\n",
    "\n",
    "        return {\n",
    "            \"query_id\": rank_result['query_id'],\n",
    "            \"query\": query,\n",
    "            \"positive_id\": positive_id,\n",
    "            \"positive\": positive_passage,\n",
    "            \"positive_label\": rank_result['positive_score'],\n",
    "            \"negative_ids\": [neg['docid'] for neg in hard_negatives],\n",
    "            \"negatives\": [self.corpus[neg['docid']] for neg in hard_negatives],\n",
    "            \"negative_labels\": [neg['score'] for neg in hard_negatives]\n",
    "        }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        queries = [item['query'] for item in batch]\n",
    "        positive_passages = [item['positive'] for item in batch]\n",
    "        positive_labels = [item['positive_label'] for item in batch]\n",
    "        negatives_flattened = [neg for item in batch for neg in item['negatives']]\n",
    "        negative_labels_flattened = [label for item in batch for label in item['negative_labels']]\n",
    "        \n",
    "        # Tokenize positives and negatives\n",
    "        tokenized_positives = self.tokenizer(queries, positive_passages, padding=True, truncation=self.truncation, return_tensors=\"pt\", max_length=self.max_seq_len)\n",
    "        repeated_queries = [query for query in queries for _ in range(self.num_neg_per_pos)]\n",
    "        tokenized_negatives = self.tokenizer(repeated_queries, negatives_flattened, padding=True, truncation=self.truncation, return_tensors=\"pt\", max_length=self.max_seq_len)\n",
    "        \n",
    "        return {\n",
    "            \"positives\": tokenized_positives,\n",
    "            \"positive_labels\": torch.tensor(positive_labels),\n",
    "            \"negatives\": tokenized_negatives,\n",
    "            \"negative_labels\": torch.tensor(negative_labels_flattened)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qids from '../data/nq-train/queries_sampled_10000.jsonl'\n",
      "Loading corpus from '../data/nq/corpus.jsonl'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "queries_path = \"../data/nq-train/queries_sampled_10000.jsonl\"\n",
    "corpus_path = \"../data/nq/corpus.jsonl\"\n",
    "negative_rank_results_path = \"../data/nq-train/nv_rerank_negatives_top100_sampled_10000_filtered.tsv\"\n",
    "positive_rank_results_path = \"../data/nq-train/nv_rerank_positives_train_sampled_10000.tsv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "dataset = PositiveNegativeDataset(queries_path, corpus_path, negative_rank_results_path, positive_rank_results_path, tokenizer, num_neg_per_pos=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "966458"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': 'train128285',\n",
       " 'query': 'rome was sacked in 410 by the goths who were led by',\n",
       " 'positive_id': 'doc1638137',\n",
       " 'positive': 'Sack of Rome (410)\\nThe Sack of Rome occurred on August 24, 410. The city was attacked by the Visigoths led by King Alaric. At that time, Rome was no longer the capital of the Western Roman Empire, having been replaced in that position first by Mediolanum in 286 and then by Ravenna in 402. Nevertheless, the city of Rome retained a paramount position as \"the eternal city\" and a spiritual center of the Empire. The sack was a major shock to contemporaries, friends and foes of the Empire alike.',\n",
       " 'positive_label': 27.171875,\n",
       " 'negative_ids': ['doc1638161'],\n",
       " 'negatives': [\"Sack of Rome (410)\\nInfuriated, Alaric broke off negotiations, and Jovius returned to Ravenna to strengthen his relationship with the Emperor. Honorius was now firmly committed to war, and Jovius swore on the Emperor's head to never to make peace with Alaric. Alaric himself soon changed his mind when he heard Honorius was attempting to recruit 10,000 Huns to fight the Goths.[65][70] He gathered a group of Roman bishops and sent them to Honorius with his new terms. He no longer sought Roman office or tribute in gold. He now only requested lands in Noricum and as much grain as the Emperor found necessary.[65] Historian Olympiodorus the Younger, writing many years later, considered these terms extremely moderate and reasonable.[69] But it was too late: Honorius' government, bound by oath and intent on war, rejected the offer. Alaric then marched on Rome.[65] The 10,000 Huns never materialized.[71]\"],\n",
       " 'negative_labels': [17.671875]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[966350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positives': {'input_ids': tensor([[     1,    328,    490,  24936,    452,  15968,   9586,    267,    262,\n",
      "            550,      2,  59801,  17841,  59801,    269,   7939,    311,   1066,\n",
      "            335,    373,    269,    411,   6294,    270,    266,  13504,    272,\n",
      "            373,    269,    264,   9586,  77212,    272,    406,    260,  97681,\n",
      "          75544,  59801,    263,   3836,    277,   3910,    265,    342,   1503,\n",
      "            264,    527,    342,    557,    482,    262,   4271,    264,    800,\n",
      "            839,  17444,    260,  59801,  25021,  77212,    263,  13936,    264,\n",
      "          10979,    283,    313,   4461,    264,    552,    315,  25845,    441,\n",
      "            342,    261,    299,    539,   2855,   1969,    267,  82756,    260,\n",
      "            344,    930,    261,  97681,   8466,  13634,   1310,    725,    268,\n",
      "            264,   1727,    283,    266,  13725,    324,    272,  77212,    295,\n",
      "          25845,    315,   9147,    267,   9239,    265,    315,    782,  24772,\n",
      "            260,   5484,  59801,  18335,    275,    386,  38922,    328,   2216,\n",
      "            264,    630,   1023,    265,   2352,    261,    901,    335,    278,\n",
      "            269,    342,    930,    264,   2674,    275,  97681,    261,    313,\n",
      "          17458,    264,  18712,    342,    385,    342,   2855,    260,  77212,\n",
      "          16902,    268,    263,   1603,    315,   2629,    264,   2648,   8584,\n",
      "          97681,    260,    643,    262,   1787,   4271,    261,  77212,  16263,\n",
      "            298,    264,  41939,    262,   2855,    775,    264,  59801,    280,\n",
      "            268,   1838,    265,   2629,    267,    417,    260,    325,    269,\n",
      "            298,    455,    416,    386,    267,   1913,    280,    268,  18963,\n",
      "            488,    264,    391,    272,    262,   2855,    284,    518,  96143,\n",
      "            260,      2,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "              0,      0,      0,      0,      0,      0,      0,      0],\n",
      "        [     1,    361,    597,    284,  12345,  20333,    335,    373,    595,\n",
      "           2674,  13089,      2,   6839,  31255,   4962,  75439,    284,   3740,\n",
      "            267,    737,   1278,   1151,    277,   2521,    265,  31801,   4726,\n",
      "            482,    266,   6544,  26866,    288,    266,   2674,   2027,    968,\n",
      "            267,    485,    920,    844,    482,  11383,    270,    262,    553,\n",
      "            280,    268,   2532,    709,    267,   1413,   1292,    260,  75439,\n",
      "           2325,    267,   1320,    275,   4726,    277,   1278,   1259,    261,\n",
      "           1151,    263,  12132,    307,   2895,   4374,    309,    260,    336,\n",
      "           2358,    284,   3170,    270,   1014,    304,  39320,    264,    903,\n",
      "            456,    261,   1151,    260,   7121,   2390, 111695,  12542,  60879,\n",
      "            272,    401,    262,   2861,  13263,    277,   1341,    261,  75439,\n",
      "            280,    268,   2774,    272,    262,   4741,   5292,    284,  32220,\n",
      "            284,   8725,    260,   2550,   4775,    592,    589,    266,   1278,\n",
      "           2043,   2658,    277,    279,   3388,    261,   4726,   1577,    272,\n",
      "          75439,    263,    342,   7434,  17895,    263,  25540,    281,    363,\n",
      "           1032,  14103,    275,    263,    338,    363,   1032,   1823,    277,\n",
      "           6839,  31255,    260,   2550,   3377,    592,   2550,   4612,    592,\n",
      "            279,   2861,   3265,    262,   3273,    457,  75439,    263,   4726,\n",
      "          13263,    277,   1555,    762,    261,   1151,    260,   2550,   5422,\n",
      "            592,    643,    262,   1061,   2861,    265,    709,    453,    261,\n",
      "          28967,    263,  18779,  42234,  40979,    327,    595,   6839,  31255,\n",
      "           2550,   4921,    592,    401,    265,   3835,  27533,    263,  44226,\n",
      "           1035,   1955,  18779,    260,   2550,   4944,    592,    344,   1370,\n",
      "           1151,    261,    393,    271,   2209,    271,    943,    271,   1371,\n",
      "          25540,  75439,  14094,   4726,    261,    304,  75439,    280,    268,\n",
      "           5970,    284,   8617,    260,   2550,   2554,    592,      2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'positive_labels': tensor([14.9453, 10.1953]), 'negatives': {'input_ids': tensor([[    1,   328,   490,  ...,     0,     0,     0],\n",
      "        [    1,   328,   490,  ...,     0,     0,     0],\n",
      "        [    1,   328,   490,  ..., 77439,   260,     2],\n",
      "        ...,\n",
      "        [    1,   361,   597,  ...,     0,     0,     0],\n",
      "        [    1,   361,   597,  ...,     0,     0,     0],\n",
      "        [    1,   361,   597,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, 'negative_labels': tensor([  2.5488,  -1.8682, -12.2344,   7.4727,  -0.7646,  -5.0977,   2.5488,\n",
      "         -0.5522, -14.9453,   1.0195,  -8.8359, -10.8750, -14.9453, -13.5859,\n",
      "        -14.9453, -12.2344])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.3906,  8.1562])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"positive_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"positives\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
