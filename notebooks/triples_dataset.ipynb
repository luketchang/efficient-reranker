{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_hits_from_qrels_queries_corpus(qrels_file, queries_file, corpus_file=None):\n",
    "    print(f\"Loading qids from '{queries_file}'\")\n",
    "    queries = load_qids_to_queries(queries_file)\n",
    "\n",
    "    print(f\"Loading corpus from '{corpus_file}'\")\n",
    "    corpus = load_pids_to_passages(corpus_file) if corpus_file is not None else None\n",
    "\n",
    "    # Step 3: Load qrels and combine all data\n",
    "    results = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        for line in f:\n",
    "            # Skip if the first line is the header\n",
    "            if line.startswith(\"query-id\"):\n",
    "                continue\n",
    "\n",
    "            qid, docid, score = line.strip().split('\\t')\n",
    "            score = float(score)\n",
    "\n",
    "            # Initialize query entry if not already present\n",
    "            if qid not in results:\n",
    "                results[qid] = {'query': queries[qid], 'hits': []}\n",
    "\n",
    "            # Create a hit entry\n",
    "            hit = {\n",
    "                'qid': qid,\n",
    "                'docid': docid,\n",
    "                'score': score,\n",
    "                'content': corpus[docid] if corpus_file is not None else None\n",
    "            }\n",
    "\n",
    "            results[qid]['hits'].append(hit)\n",
    "\n",
    "    # Step 4: Sort the queries by numeric qid and their hits by score\n",
    "    rank_results = []\n",
    "    for qid in sorted(results.keys(), key=lambda x: int(x.replace(\"test\", \"\").replace(\"train\", \"\").replace(\"dev\", \"\"))):  # Sort by numeric qid\n",
    "        sorted_hits = sorted(\n",
    "            results[qid]['hits'], \n",
    "            key=lambda x: -x['score']  # Sort hits by score in descending order\n",
    "        )\n",
    "        rank_results.append({\n",
    "            'query': results[qid]['query'],\n",
    "            'hits': sorted_hits\n",
    "        })\n",
    "\n",
    "    return rank_results\n",
    "\n",
    "def load_qids_to_queries(queries_file):\n",
    "    queries = {}\n",
    "    with open(queries_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            qid, query = line[\"_id\"], line[\"text\"]\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "\n",
    "def load_pids_to_passages(corpus_file):\n",
    "    corpus = {}\n",
    "    with open(corpus_file, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            pid = data[\"_id\"]\n",
    "            \n",
    "            # Extract title and text, combining them if the title exists\n",
    "            title = data.get(\"title\", \"\")\n",
    "            text = data[\"text\"]\n",
    "            passage = title + \"\\n\" + text if title and title.strip() else text\n",
    "            \n",
    "            corpus[pid] = passage\n",
    "    return corpus\n",
    "\n",
    "def load_qid_to_pid_to_score(qrels_file):\n",
    "    qid_to_pid_to_score = {}\n",
    "    with open(qrels_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"query-id\"):\n",
    "                continue\n",
    "\n",
    "            qid, pid, score = line.strip().split('\\t')\n",
    "            score = float(score)\n",
    "            \n",
    "            if qid not in qid_to_pid_to_score:\n",
    "                qid_to_pid_to_score[qid] = {}\n",
    "            qid_to_pid_to_score[qid][pid] = score\n",
    "    return qid_to_pid_to_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load ground truth positives\n",
    "# load top 30-50 reranked (negative) hits\n",
    "# treat dataset as one long list of hits\n",
    "#  - expand each query into its set of top k hits\n",
    "#  - len(dataset) = sum(len(hits) for hits in dataset)\n",
    "#  - getitem(idx) returns (idx // num queries) + (idx % num queries)\n",
    "\n",
    "# load qid --> queries\n",
    "# load pid --> passages\n",
    "# load rank results as qid --> pid --> score\n",
    "# load ground truth as qid --> pid --> score\n",
    "\n",
    "# get top 1000 embedding rank results (DONE)\n",
    "# send all embedding rank results to reranker (DONE)\n",
    "# remove false negatives (within 95% of lowest ground truth rerank score) (DONE)\n",
    "\n",
    "# need script that loads all query<>positive scores and sends to reranker for score (TODO)\n",
    "# pass positive qid->pid->score to dataset so it can put positive score onto rank result (TODO)\n",
    "\n",
    "# Pipeline: \n",
    "# - qrels, queries, corpus --> positive rankings (DONE: existing nv_rerank script)\n",
    "# - corpus, queries --> top 1000 embed (DONE: embed/query scripts)\n",
    "# - top 1000 embed --> top 200 reranked (DONE: nv_rerank script)\n",
    "# - top 200 rerank, positive rankings --> top 200 reranked w/out false negatives (DONE: remove_false_negatives script)\n",
    "# - top 200 reranked w/out false negatives, positive rankings --> teacher triples (TODO)\n",
    "\n",
    "# NOTE: we don't remove false negatives from rerank stage because we may still want to observe their behavior when sent through reranker or measure scoring\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "# from data_utils import load_qid_to_pid_to_score, load_pids_to_passages, load_hits_from_qrels_queries_corpus\n",
    "\n",
    "class TeacherTriplesDataset(Dataset):\n",
    "    def __init__(self, queries_path, corpus_path, negative_rank_results_path, positive_rank_results_path, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.positive_rank_results = load_qid_to_pid_to_score(positive_rank_results_path)\n",
    "        self.corpus = load_pids_to_passages(corpus_path)\n",
    "        negative_rank_results = load_hits_from_qrels_queries_corpus(negative_rank_results_path, queries_path, corpus_path)\n",
    "\n",
    "        self.negative_rank_results_with_positives = []\n",
    "        for rank_result in negative_rank_results:\n",
    "            hits = rank_result['hits']\n",
    "            qid = hits[0]['qid']\n",
    "            if qid in self.positive_rank_results:\n",
    "                for positive_id in self.positive_rank_results[qid]:\n",
    "                    positive_score = self.positive_rank_results[qid][positive_id]\n",
    "                    self.negative_rank_results_with_positives.append({\n",
    "                        \"query_id\": qid,\n",
    "                        \"query\": rank_result['query'],\n",
    "                        \"positive_id\": positive_id,\n",
    "                        \"positive_score\": positive_score,\n",
    "                        \"hits\": hits\n",
    "                    })\n",
    "\n",
    "        # Create index mapping: [(query_idx, hit_idx)]\n",
    "        self.index_mapping = []\n",
    "        for query_idx, rank_result in enumerate(self.negative_rank_results_with_positives):\n",
    "            num_hits = len(rank_result['hits'])\n",
    "            self.index_mapping.extend([(query_idx, hit_idx) for hit_idx in range(num_hits)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        query_idx, hit_idx = self.index_mapping[idx]\n",
    "        rank_result = self.negative_rank_results_with_positives[query_idx]\n",
    "        query = rank_result['query']\n",
    "        hit = rank_result['hits'][hit_idx]\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"positive_id\": rank_result['positive_id'],\n",
    "            \"positive\": self.corpus[rank_result['positive_id']],\n",
    "            \"positive_score\": rank_result['positive_score'],\n",
    "            \"negative_id\": hit['docid'],\n",
    "            \"negative\": hit['content'],\n",
    "            \"negative_score\": hit['score']\n",
    "        }\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        queries = [item['query'] for item in batch]\n",
    "        positive_passages = [item['positive'] for item in batch]\n",
    "        positive_scores = [item['positive_score'] for item in batch]\n",
    "        negative_passages = [item['negative'] for item in batch]\n",
    "        negative_scores = [item['negative_score'] for item in batch]\n",
    "\n",
    "        tokenized_positives = self.tokenizer(queries, positive_passages, padding=True, return_tensors=\"pt\")\n",
    "        tokenized_negatives = self.tokenizer(queries, negative_passages, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "        return {\n",
    "            \"positives\": tokenized_positives,\n",
    "            \"positive_scores\": torch.tensor(positive_scores),\n",
    "            \"negatives\": tokenized_negatives,\n",
    "            \"negative_scores\": torch.tensor(negative_scores)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading qids from '../data/nq-train/queries_sampled_10000.jsonl'\n",
      "Loading corpus from '../data/nq/corpus.jsonl'\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "queries_path = \"../data/nq-train/queries_sampled_10000.jsonl\"\n",
    "corpus_path = \"../data/nq/corpus.jsonl\"\n",
    "negative_rank_results_path = \"../data/nq-train/bge_en_icl_train_10k_rank_results_1000.tsv\"\n",
    "positive_rank_results_path = \"../data/nq-train/qrels/train_mapped_sampled_10000.tsv\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "dataset = TeacherTriplesDataset(queries_path, corpus_path, negative_rank_results_path, positive_rank_results_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4201000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'where was the tv show high chaparral filmed',\n",
       " 'positive_id': 'doc114945',\n",
       " 'positive': 'The High Chaparral\\nAll the exterior filming was done at Old Tucson Studios in Arizona and in the nearby Saguaro National Park, although in a few later episodes there was some filming in California and (in season 3) in the Coronado National Forest south of Tucson. The interiors were generally filmed at the NBC television studios in Burbank, Los Angeles.[1]',\n",
       " 'positive_score': 1.0,\n",
       " 'negative_id': 'doc1855892',\n",
       " 'negative': \"Hill Valley (Back to the Future)\\nFor Back to the Future Part III, Hill Valley 1885 was filmed in Sonora, California. The producers were able to use the land rent-free as long as they left the buildings there. They agreed to leave everything except the Clock Tower. Interestingly, on August 10, 1996, a lightning bolt struck the town and it burned down[citation needed]. An arson fire on the Universal Studios Hollywood backlot on November 6, 1990, had previously destroyed much of Courthouse Square, the setting in which all the other time periods were filmed. However, the Courthouse itself survived the devastation.[5] Another backlot fire on September 6, 1997. again damaged Courthouse Square. In both cases, the backlot facades were then rebuilt. The Sonora location was not rebuilt. In addition, a February 14, 1999 fire at Whittier High School, California, where some (mostly exterior) scenes were filmed,[6] destroyed the men's gym there. On June 1, 2008, yet another fire destroyed part of the rebuilt Courthouse Square backlot and damaged the clock tower.[7][8]\",\n",
       " 'negative_score': 21180.25}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[4088653]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'positives': {'input_ids': tensor([[    1,   328, 18404,  9585,   584,   280,   358,  3928,   267,   472,\n",
      "             2, 24538,   261,   273,   280,   358, 31881,   267,  1961,   307,\n",
      "          1193, 95483,   261,   273,   280,   358, 31881,   267,  1961,   309,\n",
      "           269,   266,  1710,  1223,   293, 84587,   268,   261,   263,  2841,\n",
      "           293,   733,   658,   755,  2245, 25375,  1961,  2691,   260,   325,\n",
      "           284,  1315,   267,   903,  6507,   283,   262,   883,   777,   292,\n",
      "           342,  1898, 12638,  9593, 73761,  7762,   260,     2,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [    1,   399,   269,   262,  8277,   553,   262, 14935,  6467, 12142,\n",
      "             2, 43776,  6467,  1589,   262,   813,   269,   487,   267, 14959,\n",
      "           261,   278,   284,  3325, 12142,   267, 19439,   261,   275, 80811,\n",
      "          4118,  5720,   427,   283,   266,   852,  1250,   260,  2550,  4230,\n",
      "           592,  2550,  3418,   592,  2550,  4629,   592,  2550,  4775,   592,\n",
      "          1865,  2622,   427,   267, 11383,   680,  1434,  6293,   261,   399,\n",
      "          1434,  6293,  8534,  1059,   284,   427,   283,   262,   954,   265,\n",
      "         20453,   280,   268,   563,   261,  1447, 46741,   358,  5720,   261,\n",
      "           399,  2831,  8232,  4373,   332, 12142,   261,   263, 29299,   260,\n",
      "           344,   813,   392,   261,   262,  1489,   271, 48917,  1621,   288,\n",
      "          2513, 52250,   284,   427,   283,   266, 13014,  1250,   260,  2550,\n",
      "          3377,   592,   593,  1250,   267, 14959,   284,   427,   267, 43776,\n",
      "          6467,   294,   262,  4373,  3849, 17117,   280,   268,  7334, 25023,\n",
      "           280,   268, 66290,   261,   382, 15218,   268,  2226,   280,   261,\n",
      "           332, 12142,   288,  5711,  5314, 12316,   261,  3347,  6150, 41298,\n",
      "           260, 34349,   332, 12445,   293,  5913,  1991,   260,  2550,  4612,\n",
      "           592,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}, 'positive_scores': tensor([1., 1.]), 'negatives': {'input_ids': tensor([[     1,    328,  18404,   9585,    584,    280,    358,   3928,    267,\n",
      "            472,      2,    369,    273,  15023,    280,    297,   2926,    307,\n",
      "           3088,    273,  15023,    280,    297,   2926,    309,    269,    266,\n",
      "           1710,   1223,    293,   3173,   7848,    272,    284,   3259,  16687,\n",
      "            263,   2841,    293,    279,  14878,  52119,   3265,   2492,  15097,\n",
      "            267,  14254,    260,    279,  14878,  52119,   3532,   1181,    262,\n",
      "            466,    474,    410,   2304,    777,    265,    305,    326,    275,\n",
      "            360,   1316,    705,   4896,   1758,    570,    278,    311,    265,\n",
      "            262,   4783,    354,   9461,    305,    271,   1445,   7968,    264,\n",
      "            286,   1758,    466,    705,    287,    698,    310,    285,   1468,\n",
      "           4896,   3187,    260,   2550,    435,    592,   1663,    264,   7848,\n",
      "            261,    313,  18032,    262,   1710,    416,   2069,    278,    264,\n",
      "            347,    265,    315,    774,    260,    855,    774,    280,   3688,\n",
      "            264,    262,   1710,    284,    823,  19517,   2330,    261,    304,\n",
      "            313,   3663,   1453,    277,    278,    263,    734,    278,   1181,\n",
      "            311,    265,    315,   1869,  12410,    260,   2550,    445,    592,\n",
      "              2],\n",
      "        [     1,    399,    269,    262,   8277,    553,    262,  14935,   6467,\n",
      "          12142,      2,  98396,  41839,    608,    279,   2377,    265,  43138,\n",
      "            550,    261,    336,   3765,   2538,    270,   3765,   1792,    261,\n",
      "           1315,    457,    262,    567,    263,    883,    813,    261,   4883,\n",
      "          98396,  41839,    608,    280,    268,    820,    267,    266,  15045,\n",
      "            261,    292,    359,  20666,    261,    283,   1979,    267,    299,\n",
      "          31874,    264,    262, 110487,   2538,    283,    307,   1398,  22400,\n",
      "           3251,    275,    266,  11547,   6024,    954,    309,    287,  59374,\n",
      "            262,   3765,   4140,    285,    264,    262,   1515,    265,    262,\n",
      "           1240,   4787,    267,    262,   1109,   8486,    268,    261,    283,\n",
      "            972,    293,   8211,  81769,    260,    279,    513,  52230,    265,\n",
      "            262,    550,    553,    609,   5061,    265,   4570,   1991,   1387,\n",
      "           9662,    444,    263,    275,  12967,    470,   2120,    261,  98396,\n",
      "          41839,    608,   5786,    264,    262,    609,   1240,    265,  54405,\n",
      "            261,    975,  10777,    287,  19937,    293,    264,   6651,  13981,\n",
      "            840, 104074,    268,  14309,    285,    260,      2,      0,      0,\n",
      "              0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0]])}, 'negative_scores': tensor([20525.6992, 20166.4980])}\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloaders:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     1,    328,  18404,   9585,    584,    280,    358,   3928,    267,\n",
       "            472,      2,    369,    273,  15023,    280,    297,   2926,    307,\n",
       "           3088,    273,  15023,    280,    297,   2926,    309,    269,    266,\n",
       "           1710,   1223,    293,   3173,   7848,    272,    284,   3259,  16687,\n",
       "            263,   2841,    293,    279,  14878,  52119,   3265,   2492,  15097,\n",
       "            267,  14254,    260,    279,  14878,  52119,   3532,   1181,    262,\n",
       "            466,    474,    410,   2304,    777,    265,    305,    326,    275,\n",
       "            360,   1316,    705,   4896,   1758,    570,    278,    311,    265,\n",
       "            262,   4783,    354,   9461,    305,    271,   1445,   7968,    264,\n",
       "            286,   1758,    466,    705,    287,    698,    310,    285,   1468,\n",
       "           4896,   3187,    260,   2550,    435,    592,   1663,    264,   7848,\n",
       "            261,    313,  18032,    262,   1710,    416,   2069,    278,    264,\n",
       "            347,    265,    315,    774,    260,    855,    774,    280,   3688,\n",
       "            264,    262,   1710,    284,    823,  19517,   2330,    261,    304,\n",
       "            313,   3663,   1453,    277,    278,    263,    734,    278,   1181,\n",
       "            311,    265,    315,   1869,  12410,    260,   2550,    445,    592,\n",
       "              2],\n",
       "        [     1,    399,    269,    262,   8277,    553,    262,  14935,   6467,\n",
       "          12142,      2,  98396,  41839,    608,    279,   2377,    265,  43138,\n",
       "            550,    261,    336,   3765,   2538,    270,   3765,   1792,    261,\n",
       "           1315,    457,    262,    567,    263,    883,    813,    261,   4883,\n",
       "          98396,  41839,    608,    280,    268,    820,    267,    266,  15045,\n",
       "            261,    292,    359,  20666,    261,    283,   1979,    267,    299,\n",
       "          31874,    264,    262, 110487,   2538,    283,    307,   1398,  22400,\n",
       "           3251,    275,    266,  11547,   6024,    954,    309,    287,  59374,\n",
       "            262,   3765,   4140,    285,    264,    262,   1515,    265,    262,\n",
       "           1240,   4787,    267,    262,   1109,   8486,    268,    261,    283,\n",
       "            972,    293,   8211,  81769,    260,    279,    513,  52230,    265,\n",
       "            262,    550,    553,    609,   5061,    265,   4570,   1991,   1387,\n",
       "           9662,    444,    263,    275,  12967,    470,   2120,    261,  98396,\n",
       "          41839,    608,   5786,    264,    262,    609,   1240,    265,  54405,\n",
       "            261,    975,  10777,    287,  19937,    293,    264,   6651,  13981,\n",
       "            840, 104074,    268,  14309,    285,    260,      2,      0,      0,\n",
       "              0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"positives\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
